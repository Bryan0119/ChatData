{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building unsplash-25k dataset for image search with Vector SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create and insert data to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toml import load as pload\n",
    "from clickhouse_connect import get_client\n",
    "from clickhouse_connect.driver.tools import insert_file\n",
    "\n",
    "setting = pload('../.streamlit/secrets.toml')\n",
    "\n",
    "MYSCALE_USER = setting['MYSCALE_USER']\n",
    "MYSCALE_PASSWORD = setting['MYSCALE_PASSWORD']\n",
    "MYSCALE_HOST = setting['MYSCALE_HOST']\n",
    "MYSCALE_PORT = setting['MYSCALE_PORT']\n",
    "OPENAI_API_BASE = setting['OPENAI_API_BASE']\n",
    "OPENAI_API_KEY = setting['OPENAI_API_KEY']\n",
    "\n",
    "client = get_client(host=MYSCALE_HOST, port=MYSCALE_PORT, user=MYSCALE_USER, password=MYSCALE_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download our database dump\n",
    "!mkdir -p data\n",
    "!wget https://myscale-demo.s3.ap-southeast-1.amazonaws.com/visual-dataset-explorer/unsplash_25k_clip_indexer.pq -O data/photos.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert photos with vectors and build vector index for photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0|chi-msc-1decbcc9-msc-1decbcc9-0-0', '0', '', '0', '0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "client.command('''CREATE DATABASE IF NOT EXISTS unsplash''')\n",
    "client.command('''DROP TABLE IF EXISTS unsplash.photos''')\n",
    "client.command('''\n",
    "CREATE TABLE IF NOT EXISTS unsplash.photos (\n",
    "  `photo_id` String,\n",
    "  `photo_url` String,\n",
    "  `photo_vector` Array(Float32),\n",
    "  CONSTRAINT constraint_vec_length CHECK length(photo_vector) = 512\n",
    ") ENGINE = MergeTree\n",
    "ORDER BY\n",
    "  photo_id SETTINGS index_granularity = 8192\n",
    "''')\n",
    "df = pd.read_parquet('data/photos.parquet')\n",
    "df['photo_id'] = df['id']\n",
    "df['photo_url'] = df['url']\n",
    "df['photo_vector'] = df['vector']\n",
    "df[['photo_id', 'photo_url', 'photo_vector']].to_parquet('data/exported_photos.parquet')\n",
    "insert_file(client, 'photos', 'data/exported_photos.parquet', fmt='Parquet', database='unsplash')\n",
    "client.command('ALTER TABLE unsplash.photos ADD INDEX vindx photo_vector TYPE annoy() GRANULARITY 8192')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create attribute table for photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!https_proxy=http://localhost:7890 wget -c https://unsplash-datasets.s3.amazonaws.com/lite/latest/unsplash-research-dataset-lite-latest.zip - data/attributes.zip\n",
    "!unzip -o data/attributes.zip -d data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute table\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "key_list = [\n",
    "    'photo_id',\n",
    "    'photo_featured', \n",
    "    'photo_width', \n",
    "    'photo_height', \n",
    "    'photo_aspect_ratio', \n",
    "    'photographer_username', \n",
    "    'exif_camera_make', \n",
    "    'exif_camera_model', \n",
    "    'photo_location_country', \n",
    "    'photo_location_city',\n",
    "    ]\n",
    "\n",
    "img_attr = pd.read_csv('data/photos.tsv000', delimiter='\\t')\n",
    "img_attr['photo_featured'] = np.where(img_attr['photo_featured']=='t', True, False)\n",
    "img_attr[key_list].to_parquet('data/photos_attr.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.command('''DROP TABLE IF EXISTS unsplash.photos_attributes''')\n",
    "client.command('''\n",
    "CREATE TABLE IF NOT EXISTS unsplash.photos_attributes (\n",
    "  `photo_id` String,\n",
    "  `photo_featured` Bool,\n",
    "  `photo_width` Int64,\n",
    "  `photo_height` Int64,\n",
    "  `photo_aspect_ratio` Double,\n",
    "  `photographer_username` String,\n",
    "  `exif_camera_make` Nullable(String),\n",
    "  `exif_camera_model` Nullable(String),\n",
    "  `photo_location_country` Nullable(String),\n",
    "  `photo_location_city` Nullable(String)\n",
    ") ENGINE = MergeTree\n",
    "ORDER BY\n",
    "  photo_id SETTINGS index_granularity = 8192\n",
    "''')\n",
    "df = pd.read_parquet('data/photos_attr.parquet')\n",
    "_ = insert_file(client, 'photos_attributes', 'data/photos_attr.parquet', fmt='Parquet', database='unsplash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion table\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "key_list = [\n",
    "    'converted_at',\n",
    "    'conversion_type',\n",
    "    'photo_id',\n",
    "    'anonymous_user_id',\n",
    "    'conversion_country',\n",
    "]\n",
    "img_c = pd.read_csv('data/conversions.tsv000', delimiter='\\t')\n",
    "# 2020-07-29 00:08:04.221\n",
    "img_c['converted_at'] = [dt.strptime(r.split('.')[0], '%Y-%m-%d %H:%M:%S') for r in tqdm(img_c['converted_at'])]\n",
    "img_c[key_list].to_parquet('data/photos_conversions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n",
    "img_c = pd.read_parquet('data/photos_conversions.parquet')\n",
    "batch_size = 1024\n",
    "\n",
    "client.command('''DROP TABLE IF EXISTS unsplash.photo_conversions''')\n",
    "client.command('''\n",
    "CREATE TABLE IF NOT EXISTS unsplash.photo_conversions (\n",
    "  `converted_at` DateTime,\n",
    "  `conversion_type` String,\t\n",
    "  `photo_id` String,\n",
    "  `anonymous_user_id` String,\n",
    "  `conversion_country` Nullable(String)\n",
    ") ENGINE = MergeTree\n",
    "ORDER BY\n",
    "  photo_id SETTINGS index_granularity = 8192\n",
    "''')\n",
    "\n",
    "# batch-wise insertion if you have large dataframe\n",
    "def single(n):\n",
    "    t_client = get_client(host=MYSCALE_HOST, port=MYSCALE_PORT, user=MYSCALE_USER, password=MYSCALE_PASSWORD)\n",
    "    t_client.insert_df('unsplash.photo_conversions', img_c[n:min(n+batch_size, len(img_c))])\n",
    "    t_client.close()\n",
    "\n",
    "# Using thread to obtain maximized performance\n",
    "with ThreadPool(64) as p:\n",
    "    batches = list(range(0, len(img_c), batch_size))\n",
    "    for _ in tqdm(p.imap_unordered(single, batches), total=len(batches)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_r for _r in client.query('SELECT COUNT(*) FROM unsplash.photo_conversions').named_results()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f9/bcrnh8c13g98t4x7v5jgph7w0000gp/T/ipykernel_2187/3779544801.py:27: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  metadata = MetaData(bind=engine)\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "from typing import Dict, Any\n",
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from sqlalchemy import create_engine, Column, MetaData\n",
    "from clickhouse_sqlalchemy import (\n",
    "    Table, make_session, get_declarative_base, types, engines\n",
    ")\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from chains.unsplash_chains import UnsplashSQLChain\n",
    "from prompts.unsplash_prompt import _DEFAULT_TEMPLATE\n",
    "\n",
    "from toml import load as pload\n",
    "\n",
    "setting = pload('../.streamlit/secrets.toml')\n",
    "\n",
    "MYSCALE_USER = setting['MYSCALE_USER']\n",
    "MYSCALE_PASSWORD = setting['MYSCALE_PASSWORD']\n",
    "MYSCALE_HOST = setting['MYSCALE_HOST']\n",
    "MYSCALE_PORT = setting['MYSCALE_PORT']\n",
    "OPENAI_API_BASE = setting['OPENAI_API_BASE']\n",
    "OPENAI_API_KEY = setting['OPENAI_API_KEY']\n",
    "\n",
    "engine = create_engine(\n",
    "    f'clickhouse://{MYSCALE_USER}:{MYSCALE_PASSWORD}@{MYSCALE_HOST}:{MYSCALE_PORT}/unsplash?protocol=https')\n",
    "metadata = MetaData(bind=engine)\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=['top_k', 'table_info', 'input'],\n",
    "                        template=_DEFAULT_TEMPLATE)\n",
    "\n",
    "def get_key():\n",
    "    with open('key.txt') as f:\n",
    "        keys = [l.split('\\n')[0] for l in f.readlines() if l[:3] == 'sk-']\n",
    "    return keys[0]\n",
    "\n",
    "environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "environ['OPENAI_API_BASE'] = OPENAI_API_BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from typing import List\n",
    "from langchain import SQLDatabase, OpenAI\n",
    "from langchain.chains.sql_database.base import SQLDatabaseChain\n",
    "from langchain.chains.sql_database.parser import VectorSQLOutputParser\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "from transformers import CLIPTokenizerFast, CLIPModel\n",
    "class EmbModel(Embeddings):\n",
    "    def __init__(self, model_name = \"openai/clip-vit-base-patch32\") -> None:\n",
    "        model_name = \"openai/clip-vit-base-patch32\"\n",
    "        self.tokenizer = CLIPTokenizerFast.from_pretrained(model_name)\n",
    "        self.clip = CLIPModel.from_pretrained(model_name)\n",
    "    \n",
    "    def embed_query(self, prompt: str, tokenizer, clip):\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        out = clip.get_text_features(**inputs)\n",
    "        xq = out.squeeze(0).cpu().detach().numpy().tolist()\n",
    "        return xq\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return super().embed_query(texts)\n",
    "\n",
    "chain = SQLDatabaseChain.from_llm(llm=OpenAI(temperature=0), prompt=PROMPT, verbose=True, \n",
    "                                  db=SQLDatabase(engine, None, metadata), \n",
    "                                  sql_cmd_parser=VectorSQLOutputParser(\n",
    "                                      model=EmbModel(model_name=\"openai/clip-vit-base-patch32\")),\n",
    "                                  return_direct=True)\n",
    "chain.verbose = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sql(question):\n",
    "    _sql_str = chain.run(question)\n",
    "    for n in ['This is not a valid question.']:\n",
    "        if n in _sql_str:\n",
    "            return 'cannot-parse'\n",
    "    start = _sql_str.find('NeuralArray(')\n",
    "    if start > 0:\n",
    "        _matched = _sql_str[_sql_str.find('NeuralArray(')+len('NeuralArray('):]\n",
    "        entity = _matched[:_matched.find(')')]\n",
    "        end = _matched.find(')') + start + len('NeuralArray(') + 1\n",
    "        vecs = prompt2vec(entity, tokenizer, clip)\n",
    "        vecs_str = '[' + ','.join(map(str, vecs)) + ']'\n",
    "        _sql_str_compl = _sql_str.replace('DISTANCE', 'cosineDistance').replace(_sql_str[start:end], vecs_str)\n",
    "        if _sql_str_compl[-1] == ';':\n",
    "            _sql_str_compl = _sql_str_compl[:-1]\n",
    "    else:\n",
    "        _sql_str_compl = _sql_str\n",
    "    try:\n",
    "        r = client.query(_sql_str_compl).named_results()\n",
    "    except Exception as e:\n",
    "        return 'cannot-execute'\n",
    "    return [_r for _r in r]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"what is the photo that has the most downloads which was taken by davidclode?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioned Vector Search SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_sql(\"what is the most-5 similar photos's url to dog which was shot in Australia?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complicated entity SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_sql(\"what is the most-10 similar photo to an entity called 'a lake by a house'? And what are their numbers of download?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complicated entity SQL with implicit condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_sql(\"what is the most-10 similar photo to an entity called 'a lake by a house' which is a square photo?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-By clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_sql(\"what are the most popular authors?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "q = [json.load(open(f))['question'] for f in glob('log/*.json')]\n",
    "result = {}\n",
    "for _q in q:\n",
    "    result[_q] = natural_sql(_q)\n",
    "    print(result[_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([k for k, v in result.items() if v == 'cannot-execute'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "# what does the prompt look like?\n",
    "llm_chain = LLMChain(llm=chain.llm, prompt=chain.prompt)\n",
    "llm_inputs = {\n",
    "    \"input\": \"what is the closet id to dog whose bounding box width and height is smaller than 0.5?\",\n",
    "    \"top_k\": chain.top_k,\n",
    "    \"dialect\": chain.database.dialect,\n",
    "    \"table_info\": chain.database.get_table_info(table_names=None),\n",
    "    \"stop\": [\"\\nSQLQuery:\"],\n",
    "}\n",
    "print(llm_chain.prep_prompts([llm_inputs])[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace neural array with embeddings\n",
    "import re\n",
    "from lexer import Lexer, Rule, Token\n",
    "\n",
    "text_sql = \" SELECT obj_id FROM vector_database WHERE box_w < 0.5 AND box_h < 0.5 ORDER BY DISTANCE(prelogit, NeuralArray(dog)) LIMIT 5\"\n",
    "\n",
    "lexer = Lexer(\n",
    "    rules=[\n",
    "        Rule('SELECT', re.compile('\\s*(SELECT|select)\\s+\\w+\\s+')),\n",
    "        Rule('FROM', re.compile('(FROM|from)\\s+\\w+\\s+')),\n",
    "        Rule('WHERE', re.compile('(WHERE|where)\\s+\\w+\\s*((\\!\\=)|[\\>\\<\\=])\\s*[\\w\\d\\.\\']+(\\s+(AND|and)\\s+\\w+\\s*((\\!\\=)|[\\>\\<\\=])\\s*[\\w\\d\\.\\']+\\s*)*\\s+')),\n",
    "        Rule('ORDERBY', re.compile('(ORDER\\s+BY|order\\s+by)\\s+[\\w]*\\(*[\\w\\,\\s\\(\\)]+\\)\\s+'),\n",
    "             next=[\n",
    "                 Rule('clause', re.compile('(ORDER\\s+BY|order\\s+by)\\s+')),\n",
    "                 Rule('expr', re.compile('\\w+\\([\\w\\s\\,\\(\\)]+\\)'), next=[\n",
    "                     Rule('op', re.compile('\\w+\\(')),\n",
    "                     Rule('col', re.compile('\\w+\\,')),\n",
    "                     Rule('Narr', re.compile('\\s*NeuralArray\\(')),\n",
    "                     Rule('entity', re.compile('\\s*\\w+')),\n",
    "                     Rule(')', re.compile('\\)*')),\n",
    "                     ])\n",
    "                 ]),\n",
    "        Rule('LIMIT', re.compile('(LIMIT|limit)\\s+\\d'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "def token2lrtree(tokens):\n",
    "    return {t.identifier: (token2lrtree(t.content)) if type(t.content) is list else t.content for i, t in enumerate(tokens)}\n",
    "\n",
    "t = lexer.lex(text_sql)\n",
    "d = token2lrtree(t[0])\n",
    "print(d)\n",
    "\n",
    "# for n in t[0]:\n",
    "#     if n.identifier == 'ORDERBY':\n",
    "        \n",
    "#         print(n.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
